{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Introduction to Hadoop MapReduce -- Debugging </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Movie Ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Data: Movie Ratings and Recommendation **\n",
    "\n",
    "An independent movie company is looking to invest in a new movie project. With limited finance, the company wants to \n",
    "analyze the reaction of audiences, particularly toward various movie genres, in order to identify beneficial \n",
    "movie project to focus on. The company relies on data collected from a publicly available recommendation service \n",
    "by [MovieLens](http://dl.acm.org/citation.cfm?id=2827872). This \n",
    "[dataset](http://files.grouplens.org/datasets/movielens/ml-10m-README.html) contains **24404096** ratings and **668953**\n",
    " tag applications across **40110** movies. These data were created by **247753** users between January 09, 1995 and January 29, 2016. This dataset was generated on October 17, 2016. \n",
    "\n",
    "From this dataset, several analyses are possible, include the followings:\n",
    "1.   Find movies which have the highest average ratings over the years and identify the corresponding genre.\n",
    "2.   Find genres which have the highest average ratings over the years.\n",
    "3.   Find users who rate movies most frequently in order to contact them for in-depth marketing analysis.\n",
    "\n",
    "These types of analyses, which are somewhat ambiguous, demand the ability to quickly process large amount of data in \n",
    "elatively short amount of time for decision support purposes. In these situations, the sizes of the data typically \n",
    "make analysis done on a single machine impossible and analysis done using a remote storage system impractical. For \n",
    "remainder of the lessons, we will learn how HDFS provides the basis to store massive amount of data and to enable \n",
    "the programming approach to analyze these data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!module add hdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cypress-kinit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 items\r\n",
      "-rw-r--r--   2 lngo hdfs-user      9.3 K 2017-03-15 09:49 /repository/movielens/README.txt\r\n",
      "-rw-r--r--   2 lngo hdfs-user    317.9 M 2017-03-15 09:49 /repository/movielens/genome-scores.csv\r\n",
      "-rw-r--r--   2 lngo hdfs-user     17.7 K 2017-03-15 09:49 /repository/movielens/genome-tags.csv\r\n",
      "-rw-r--r--   2 lngo hdfs-user    839.2 K 2017-03-15 09:49 /repository/movielens/links.csv\r\n",
      "-rw-r--r--   2 lngo hdfs-user      1.9 M 2017-03-15 09:49 /repository/movielens/movies.csv\r\n",
      "-rw-r--r--   2 lngo hdfs-user    632.7 M 2017-03-15 09:49 /repository/movielens/ratings.csv\r\n",
      "-rw-r--r--   2 lngo hdfs-user     22.9 M 2017-03-15 09:49 /repository/movielens/tags.csv\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls -h /repository/movielens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find movies which have the highest average ratings over the years and report their ratings and genres\n",
    "\n",
    "- Find the average ratings of all movies over the years\n",
    "- Sort the average ratings from highest to lowest\n",
    "- Report the results, augmented by genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting meanGenreMapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile meanGenreMapper.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "import json\n",
    "\n",
    "# for nonHDFS run\n",
    "# movieFile = \"./movielens/movies.csv\"\n",
    "\n",
    "# for HDFS run\n",
    "movieFile = \"./movies.csv\"\n",
    "\n",
    "movieList = {}\n",
    "genreList = {}\n",
    "\n",
    "with open(movieFile, mode = 'r') as infile:\n",
    "    reader = csv.reader(infile)\n",
    "    for row in reader:\n",
    "        movieList[row[0]] = {}\n",
    "        movieList[row[0]][\"title\"] = row[1]\n",
    "        movieList[row[0]][\"genre\"] = row[2]\n",
    "\n",
    "for oneMovie in sys.stdin:\n",
    "    oneMovie = oneMovie.strip()\n",
    "    ratingInfo = oneMovie.split(\",\")\n",
    "    try:\n",
    "        genres = movieList[ratingInfo[1]][\"genre\"]\n",
    "        rating = float(ratingInfo[2])\n",
    "        for genre in genres.split(\"|\"):\n",
    "            if genre in genreList:\n",
    "                genreList[genre][\"total_rating\"] += rating\n",
    "                genreList[genre][\"total_count\"] += 1\n",
    "            else:\n",
    "                genreList[genre] = {}\n",
    "                genreList[genre][\"total_rating\"] = rating\n",
    "                genreList[genre][\"total_count\"] = 1\n",
    "    except ValueError:\n",
    "        continue\n",
    "        \n",
    "for genre in genreList:\n",
    "    print (\"%s\\t%s\" % (genre, json.dumps(genreList[genre])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action\t{\"total_count\": 1, \"total_rating\": 1.0}\r\n",
      "Comedy\t{\"total_count\": 2, \"total_rating\": 6.0}\r\n",
      "Crime\t{\"total_count\": 1, \"total_rating\": 5.0}\r\n",
      "Drama\t{\"total_count\": 1, \"total_rating\": 5.0}\r\n",
      "Romance\t{\"total_count\": 2, \"total_rating\": 6.0}\r\n",
      "Sci-Fi\t{\"total_count\": 1, \"total_rating\": 1.0}\r\n",
      "Thriller\t{\"total_count\": 1, \"total_rating\": 1.0}\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /repository/movielens/ratings.csv \\\n",
    "    2>/dev/null | head -n 5 | python meanGenreMapper.py | sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting meanGenreReducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile meanGenreReducer.py\n",
    "#!/usr/bin/env python\n",
    "import sys\n",
    "import json\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    genre, ratingString = line.split(\"\\t\", 1)\n",
    "    ratingInfo = json.loads(ratingString)\n",
    "    \n",
    "    rating_sum = ratingInfo[\"total_rating\"]\n",
    "    rating_count = ratingInfo[\"total_count\"]\n",
    "\n",
    "    rating_average = rating_sum / rating_count\n",
    "    print (\"%s\\t%s\" % (genre, rating_average))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action\t3.611111111111111\r\n",
      "Adventure\t3.6334745762711864\r\n",
      "Animation\t3.8511904761904763\r\n",
      "Children\t3.6458333333333335\r\n",
      "Comedy\t3.607769423558897\r\n",
      "Crime\t3.7708333333333335\r\n",
      "Documentary\t3.3333333333333335\r\n",
      "Drama\t3.759023354564756\r\n",
      "Fantasy\t3.6313559322033897\r\n",
      "Film-Noir\t4.1\r\n",
      "Horror\t3.1714285714285713\r\n",
      "IMAX\t3.9054054054054053\r\n",
      "Musical\t3.6475409836065573\r\n",
      "Mystery\t3.6339285714285716\r\n",
      "Romance\t3.7320143884892087\r\n",
      "Sci-Fi\t3.4507575757575757\r\n",
      "Thriller\t3.5379464285714284\r\n",
      "War\t3.8545454545454545\r\n",
      "Western\t3.5\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /repository/movielens/ratings.csv 2>/dev/null \\\n",
    "    | head -n 1000 \\\n",
    "    | python meanGenreMapper.py \\\n",
    "    | sort \\\n",
    "    | python meanGenreReducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action\t3.4544531514134182\r\n",
      "Adventure\t3.507091937178361\r\n",
      "Animation\t3.610498964247507\r\n",
      "Children\t3.4166406309798836\r\n",
      "Comedy\t3.4174603547910225\r\n",
      "Crime\t3.6785019629867692\r\n",
      "Documentary\t3.722772310125531\r\n",
      "Drama\t3.6742773734853547\r\n",
      "Fantasy\t3.5029912314315985\r\n",
      "Film-Noir\t3.9408055354000218\r\n",
      "Horror\t3.275260214307868\r\n",
      "IMAX\t3.637097658372336\r\n",
      "Musical\t3.5439135923140777\r\n",
      "Mystery\t3.6615097685590157\r\n",
      "(no genres listed)\t3.208014943114281\r\n",
      "Romance\t3.5424619995390905\r\n",
      "Sci-Fi\t3.455172838867968\r\n",
      "Thriller\t3.5126902934511794\r\n",
      "War\t3.8032667825565434\r\n",
      "Western\t3.571619485587279\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /repository/movielens/ratings.csv 2>/dev/null \\\n",
    "    | python meanGenreMapper.py \\\n",
    "    | sort \\\n",
    "    | python meanGenreReducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting genreMapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile genreMapper.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "import json\n",
    "import statistics\n",
    "\n",
    "# for nonHDFS run\n",
    "# movieFile = \"./movielens/movies.csv\"\n",
    "\n",
    "# for HDFS run\n",
    "movieFile = \"./movies.csv\"\n",
    "\n",
    "movieList = {}\n",
    "genreList = {}\n",
    "\n",
    "with open(movieFile, mode = 'r') as infile:\n",
    "    reader = csv.reader(infile)\n",
    "    for row in reader:\n",
    "        movieList[row[0]] = {}\n",
    "        movieList[row[0]][\"title\"] = row[1]\n",
    "        movieList[row[0]][\"genre\"] = row[2]\n",
    "\n",
    "for oneMovie in sys.stdin:\n",
    "    oneMovie = oneMovie.strip()\n",
    "    ratingInfo = oneMovie.split(\",\")\n",
    "    try:\n",
    "        genres = movieList[ratingInfo[1]][\"genre\"]\n",
    "        rating = float(ratingInfo[2])\n",
    "        for genre in genres.split(\"|\"):\n",
    "            if genre not in genreList:\n",
    "                genreList[genre] = []\n",
    "            genreList[genre].append(rating)\n",
    "    except ValueError:\n",
    "        continue\n",
    "\n",
    "genreInfoDict = {}        \n",
    "for genre in genreList:\n",
    "    genreInfoDict[genre] = {}\n",
    "    genreInfoDict[genre][\"mean\"] = sum(genreList[genre]) / len(genreList[genre])\n",
    "    genreInfoDict[genre][\"median\"] = statistics.median(genreList[genre])\n",
    "    genreInfoDict[genre][\"stdev\"] = statistics.stdev(genreList[genre])\n",
    "    print (\"%s\\t%s\" % (genre, json.dumps(genreInfoDict[genre])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action\t{\"median\": 3.5, \"stdev\": 0.9462024864156431, \"mean\": 3.611111111111111}\r\n",
      "Adventure\t{\"median\": 4.0, \"stdev\": 0.8740844470801887, \"mean\": 3.6334745762711864}\r\n",
      "Animation\t{\"median\": 4.0, \"stdev\": 0.7354195667680543, \"mean\": 3.8511904761904763}\r\n",
      "Children\t{\"median\": 4.0, \"stdev\": 0.838655970905242, \"mean\": 3.6458333333333335}\r\n",
      "Comedy\t{\"median\": 3.5, \"stdev\": 0.8971841975811686, \"mean\": 3.607769423558897}\r\n",
      "Crime\t{\"median\": 4.0, \"stdev\": 0.882770194775071, \"mean\": 3.7708333333333335}\r\n",
      "Documentary\t{\"median\": 4.0, \"stdev\": 2.081665999466133, \"mean\": 3.3333333333333335}\r\n",
      "Drama\t{\"median\": 4.0, \"stdev\": 0.8805210627841876, \"mean\": 3.759023354564756}\r\n",
      "Fantasy\t{\"median\": 4.0, \"stdev\": 0.8732197178535702, \"mean\": 3.6313559322033897}\r\n",
      "Film-Noir\t{\"median\": 4.0, \"stdev\": 0.22360679774997896, \"mean\": 4.1}\r\n",
      "Horror\t{\"median\": 3.5, \"stdev\": 1.1627987101541413, \"mean\": 3.1714285714285713}\r\n",
      "IMAX\t{\"median\": 4.0, \"stdev\": 0.7438436013728311, \"mean\": 3.9054054054054053}\r\n",
      "Musical\t{\"median\": 4.0, \"stdev\": 1.0462005157357184, \"mean\": 3.6475409836065573}\r\n",
      "Mystery\t{\"median\": 3.5, \"stdev\": 0.8338905063756132, \"mean\": 3.6339285714285716}\r\n",
      "Romance\t{\"median\": 4.0, \"stdev\": 0.8309992939269115, \"mean\": 3.7320143884892087}\r\n",
      "Sci-Fi\t{\"median\": 3.5, \"stdev\": 1.001639588098427, \"mean\": 3.4507575757575757}\r\n",
      "Thriller\t{\"median\": 3.5, \"stdev\": 0.8932418293648692, \"mean\": 3.5379464285714284}\r\n",
      "War\t{\"median\": 4.0, \"stdev\": 0.900990850000438, \"mean\": 3.8545454545454545}\r\n",
      "Western\t{\"median\": 3.5, \"stdev\": 1.0690449676496976, \"mean\": 3.5}\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /repository/movielens/ratings.csv 2>/dev/null \\\n",
    "    | head -n 1000 \\\n",
    "    | python genreMapper.py \\\n",
    "    | sort \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting medianGenreReducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile medianGenreReducer.py\n",
    "#!/usr/bin/env python\n",
    "import sys\n",
    "import statistics\n",
    "import json\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    genre, ratingString = line.split(\"\\t\", 1)\n",
    "    ratingInfo = json.loads(ratingString)\n",
    "    \n",
    "    rating_median = ratingInfo[\"median\"]\n",
    "    print (\"%s\\t%s\" % (genre, rating_median))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action\t3.5\r\n",
      "Adventure\t4.0\r\n",
      "Animation\t4.0\r\n",
      "Children\t4.0\r\n",
      "Comedy\t3.5\r\n",
      "Crime\t4.0\r\n",
      "Documentary\t4.0\r\n",
      "Drama\t4.0\r\n",
      "Fantasy\t4.0\r\n",
      "Film-Noir\t4.0\r\n",
      "Horror\t3.5\r\n",
      "IMAX\t4.0\r\n",
      "Musical\t4.0\r\n",
      "Mystery\t3.5\r\n",
      "Romance\t4.0\r\n",
      "Sci-Fi\t3.5\r\n",
      "Thriller\t3.5\r\n",
      "War\t4.0\r\n",
      "Western\t3.5\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /repository/movielens/ratings.csv 2>/dev/null \\\n",
    "    | head -n 1000 \\\n",
    "    | python genreMapper.py \\\n",
    "    | sort \\\n",
    "    | python medianGenreReducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action\t3.5\r\n",
      "Adventure\t3.5\r\n",
      "Animation\t4.0\r\n",
      "Children\t3.5\r\n",
      "Comedy\t3.5\r\n",
      "Crime\t4.0\r\n",
      "Documentary\t4.0\r\n",
      "Drama\t4.0\r\n",
      "Fantasy\t3.5\r\n",
      "Film-Noir\t4.0\r\n",
      "Horror\t3.5\r\n",
      "IMAX\t4.0\r\n",
      "Musical\t4.0\r\n",
      "Mystery\t4.0\r\n",
      "(no genres listed)\t3.5\r\n",
      "Romance\t4.0\r\n",
      "Sci-Fi\t3.5\r\n",
      "Thriller\t3.5\r\n",
      "War\t4.0\r\n",
      "Western\t4.0\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /repository/movielens/ratings.csv 2>/dev/null \\\n",
    "    | python genreMapper.py \\\n",
    "    | sort \\\n",
    "    | python medianGenreReducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting stdevGenreReducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile stdevGenreReducer.py\n",
    "#!/usr/bin/env python\n",
    "import sys\n",
    "import statistics\n",
    "import json\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    genre, ratingString = line.split(\"\\t\", 1)\n",
    "    ratingInfo = json.loads(ratingString)\n",
    "    \n",
    "    rating_stdev = ratingInfo[\"stdev\"]\n",
    "    print (\"%s\\t%s\" % (genre, rating_stdev))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action\t0.9462024864156431\r\n",
      "Adventure\t0.8740844470801887\r\n",
      "Animation\t0.7354195667680543\r\n",
      "Children\t0.838655970905242\r\n",
      "Comedy\t0.8971841975811686\r\n",
      "Crime\t0.882770194775071\r\n",
      "Documentary\t2.081665999466133\r\n",
      "Drama\t0.8805210627841876\r\n",
      "Fantasy\t0.8732197178535702\r\n",
      "Film-Noir\t0.22360679774997896\r\n",
      "Horror\t1.1627987101541413\r\n",
      "IMAX\t0.7438436013728311\r\n",
      "Musical\t1.0462005157357184\r\n",
      "Mystery\t0.8338905063756132\r\n",
      "Romance\t0.8309992939269115\r\n",
      "Sci-Fi\t1.001639588098427\r\n",
      "Thriller\t0.8932418293648692\r\n",
      "War\t0.900990850000438\r\n",
      "Western\t1.0690449676496976\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /repository/movielens/ratings.csv 2>/dev/null \\\n",
    "    | head -n 1000 \\\n",
    "    | python genreMapper.py \\\n",
    "    | sort \\\n",
    "    | python stdevGenreReducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action\t1.0721365559782599\n",
      "Adventure\t1.0675220659603486\n",
      "Animation\t1.0317178473795843\n",
      "Children\t1.1010527199833091\n",
      "Comedy\t1.0850123560000309\n",
      "Crime\t1.0132314135781613\n",
      "Documentary\t1.0220074103047196\n",
      "Drama\t1.002524350894216\n",
      "Fantasy\t1.0867500247329882\n",
      "Film-Noir\t0.9154781611402477\n",
      "Horror\t1.1520617325923062\n",
      "IMAX\t1.0274731139134385\n",
      "Musical\t1.0627111413546169\n",
      "Mystery\t1.0119406716579022\n",
      "(no genres listed)\t1.2310507794221146\n",
      "Romance\t1.046780167005858\n",
      "Sci-Fi\t1.0916316918987354\n",
      "Thriller\t1.0398685086065347\n",
      "War\t0.9968902171996887\n",
      "Western\t1.0256165325514484\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /repository/movielens/ratings.csv 2>/dev/null \\\n",
    "    | python genreMapper.py \\\n",
    "    | sort \\\n",
    "    | python stdevGenreReducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting genreReducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile genreReducer.py\n",
    "#!/usr/bin/env python\n",
    "import sys\n",
    "import statistics\n",
    "import json\n",
    "\n",
    "print(\"Genre\\t\\tMean\\t\\tMedian\\t\\tStandard deviation\")\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    genre, ratingString = line.split(\"\\t\", 1)\n",
    "    ratingInfo = json.loads(ratingString)\n",
    "    \n",
    "    rating_mean = ratingInfo[\"mean\"]\n",
    "    rating_median = ratingInfo[\"stdev\"]\n",
    "    rating_stdev = ratingInfo[\"stdev\"]\n",
    "    print (\"%s\\t\\t%.4f\\t\\t%s\\t\\t%.4f\" % (genre, rating_mean, rating_median, rating_stdev))  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genre\t\tMean\t\tMedian\t\tStandard deviation\n",
      "Action\t\t3.6111\n",
      "Adventure\t\t3.6335\n",
      "Animation\t\t3.8512\n",
      "Children\t\t3.6458\n",
      "Comedy\t\t3.6078\n",
      "Crime\t\t3.7708\n",
      "Documentary\t\t3.3333\n",
      "Drama\t\t3.7590\n",
      "Fantasy\t\t3.6314\n",
      "Film-Noir\t\t4.1000\n",
      "Horror\t\t3.1714\n",
      "IMAX\t\t3.9054\n",
      "Musical\t\t3.6475\n",
      "Mystery\t\t3.6339\n",
      "Romance\t\t3.7320\n",
      "Sci-Fi\t\t3.4508\n",
      "Thriller\t\t3.5379\n",
      "War\t\t3.8545\n",
      "Western\t\t3.5000\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /repository/movielens/ratings.csv 2>/dev/null \\\n",
    "    | head -n 1000 \\\n",
    "    | python genreMapper.py \\\n",
    "    | sort \\\n",
    "    | python genreReducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genre\t\tMean\t\tMedian\t\tStandard deviation\n",
      "Action\t\t3.4545\t\t3.5\t\t1.0721\n",
      "Adventure\t\t3.5071\t\t3.5\t\t1.0675\n",
      "Animation\t\t3.6105\t\t4.0\t\t1.0317\n",
      "Children\t\t3.4166\t\t3.5\t\t1.1011\n",
      "Comedy\t\t3.4175\t\t3.5\t\t1.0850\n",
      "Crime\t\t3.6785\t\t4.0\t\t1.0132\n",
      "Documentary\t\t3.7228\t\t4.0\t\t1.0220\n",
      "Drama\t\t3.6743\t\t4.0\t\t1.0025\n",
      "Fantasy\t\t3.5030\t\t3.5\t\t1.0868\n",
      "Film-Noir\t\t3.9408\t\t4.0\t\t0.9155\n",
      "Horror\t\t3.2753\t\t3.5\t\t1.1521\n",
      "IMAX\t\t3.6371\t\t4.0\t\t1.0275\n",
      "Musical\t\t3.5439\t\t4.0\t\t1.0627\n",
      "Mystery\t\t3.6615\t\t4.0\t\t1.0119\n",
      "(no genres listed)\t\t3.2080\t\t3.5\t\t1.2311\n",
      "Romance\t\t3.5425\t\t4.0\t\t1.0468\n",
      "Sci-Fi\t\t3.4552\t\t3.5\t\t1.0916\n",
      "Thriller\t\t3.5127\t\t3.5\t\t1.0399\n",
      "War\t\t3.8033\t\t4.0\t\t0.9969\n",
      "Western\t\t3.5716\t\t4.0\t\t1.0256\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /repository/movielens/ratings.csv 2>/dev/null \\\n",
    "    | python genreMapper.py \\\n",
    "    | sort \\\n",
    "    | python genreReducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting userMapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile userMapper.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "import json\n",
    "\n",
    "movieFile = \"movies.csv\"\n",
    "movieList = {}\n",
    "\n",
    "with open(movieFile, mode = 'r') as infile:\n",
    "    reader = csv.reader(infile)\n",
    "    for row in reader:\n",
    "        movieList[row[0]] = {}\n",
    "        movieList[row[0]][\"title\"] = row[1]\n",
    "        movieList[row[0]][\"genre\"] = row[2].strip().split(\"|\")\n",
    "\n",
    "userDict = {}\n",
    "        \n",
    "for oneMovie in sys.stdin:\n",
    "    oneMovie = oneMovie.strip()\n",
    "    ratingInfo = oneMovie.split(\",\")\n",
    "    try:\n",
    "        user = int(ratingInfo[0])\n",
    "        movieTitle = movieList[ratingInfo[1]][\"title\"]\n",
    "        movieGenre = movieList[ratingInfo[1]][\"genre\"]\n",
    "        rating = float(ratingInfo[2])\n",
    "        \n",
    "        if user in userDict:\n",
    "            userDict[user][\"count\"] += 1            \n",
    "        else:\n",
    "            userDict[user] = {}\n",
    "            userDict[user][\"count\"] = 1\n",
    "            userDict[user][\"genre\"] = {}\n",
    "        for genre in movieGenre:\n",
    "            if genre not in userDict[user][\"genre\"]:\n",
    "                userDict[user][\"genre\"][genre] = 0\n",
    "            userDict[user][\"genre\"][genre] += 1\n",
    "    except ValueError:\n",
    "        continue\n",
    "        \n",
    "for user in userDict:\n",
    "    print(\"%s\\t%s\" % (user, json.dumps(userDict[user])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t{\"count\": 7, \"genre\": {\"Crime\": 1, \"Thriller\": 2, \"Comedy\": 2, \"Sci-Fi\": 1, \"Romance\": 2, \"Horror\": 1, \"Action\": 1, \"Drama\": 3}}\r\n",
      "2\t{\"count\": 10, \"genre\": {\"Crime\": 1, \"Thriller\": 4, \"Comedy\": 3, \"Adventure\": 2, \"Romance\": 2, \"Sci-Fi\": 1, \"Mystery\": 1, \"Action\": 4, \"War\": 1, \"Drama\": 4}}\r\n",
      "3\t{\"count\": 62, \"genre\": {\"Crime\": 8, \"Comedy\": 24, \"Romance\": 17, \"Horror\": 1, \"Action\": 16, \"Fantasy\": 6, \"Drama\": 27, \"Western\": 3, \"Thriller\": 26, \"Mystery\": 3, \"Adventure\": 17, \"Children\": 13, \"IMAX\": 3, \"Sci-Fi\": 3, \"Animation\": 7, \"War\": 2, \"Musical\": 8}}\r\n",
      "4\t{\"count\": 1, \"genre\": {\"Comedy\": 1, \"Adventure\": 1, \"Children\": 1, \"Musical\": 1}}\r\n",
      "5\t{\"count\": 19, \"genre\": {\"Crime\": 5, \"Comedy\": 8, \"Romance\": 2, \"Action\": 7, \"Fantasy\": 1, \"Drama\": 7, \"Thriller\": 7, \"Mystery\": 2, \"Adventure\": 8, \"Film-Noir\": 1, \"Sci-Fi\": 5, \"War\": 2, \"Musical\": 1}}\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /repository/movielens/ratings.csv 2>/dev/null \\\n",
    "    | head -n 100 \\\n",
    "    | python userMapper.py \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t{\"genre\": {\"Horror\": 1, \"Action\": 1, \"Drama\": 3, \"Comedy\": 2, \"Sci-Fi\": 1, \"Romance\": 2, \"Crime\": 1, \"Thriller\": 2}, \"count\": 7}\r\n",
      "2\t{\"genre\": {\"Mystery\": 1, \"Action\": 4, \"Drama\": 4, \"Sci-Fi\": 1, \"Romance\": 2, \"Comedy\": 3, \"Crime\": 1, \"Adventure\": 2, \"War\": 1, \"Thriller\": 4}, \"count\": 10}\r\n",
      "3\t{\"genre\": {\"Mystery\": 3, \"Children\": 13, \"Drama\": 27, \"Musical\": 8, \"Animation\": 7, \"Crime\": 8, \"Adventure\": 17, \"IMAX\": 3, \"Thriller\": 26, \"War\": 2, \"Sci-Fi\": 3, \"Comedy\": 24, \"Fantasy\": 6, \"Romance\": 17, \"Western\": 3, \"Horror\": 1, \"Action\": 16}, \"count\": 62}\r\n",
      "4\t{\"genre\": {\"Children\": 1, \"Comedy\": 1, \"Adventure\": 1, \"Musical\": 1}, \"count\": 1}\r\n",
      "5\t{\"genre\": {\"Mystery\": 2, \"Action\": 7, \"Drama\": 7, \"Film-Noir\": 1, \"Crime\": 5, \"Adventure\": 8, \"War\": 2, \"Fantasy\": 1, \"Musical\": 1, \"Sci-Fi\": 5, \"Thriller\": 7, \"Romance\": 2, \"Comedy\": 8}, \"count\": 19}\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /repository/movielens/ratings.csv 2>/dev/null \\\n",
    "    | head -n 100 \\\n",
    "    | python userMapper.py \\\n",
    "    | sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting userReducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile userReducer.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "import json\n",
    "\n",
    "mostRatingUser = None\n",
    "mostRatingCount = 0\n",
    "genreDict = None\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    user, genreString = line.split(\"\\t\", 1)\n",
    "    genreInfo = json.loads(genreString)\n",
    "\n",
    "    if not mostRatingUser or genreInfo[\"count\"] > mostRatingCount:\n",
    "        mostRatingUser = user\n",
    "        mostRatingCount = genreInfo[\"count\"] \n",
    "        genreDict = genreInfo[\"genre\"]\n",
    "        \n",
    "# print(genreDict)\n",
    "\n",
    "mostRatedCount = 0\n",
    "mostRatedGenre = None\n",
    "\n",
    "for genre, count in genreDict.items():\n",
    "    if count > mostRatedCount:\n",
    "        mostRatedCount = count\n",
    "        mostRatedGenre = genre\n",
    "    \n",
    "print(\"%s -- Total Rating Counts: %d -- Most Rated Genre: %s - %d\" % (mostRatingUser, mostRatingCount, mostRatedGenre, mostRatedCount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 -- Total Rating Counts: 252 -- Most Rated Genre: Drama - 145\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /repository/movielens/ratings.csv 2>/dev/null \\\n",
    "    | head -n 1000 \\\n",
    "    | python userMapper.py \\\n",
    "    | sort \\\n",
    "    | python userReducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186590 -- Total Rating Counts: 13250 -- Most Rated Genre: Drama - 8026\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /repository/movielens/ratings.csv 2>/dev/null \\\n",
    "    | python userMapper.py \\\n",
    "    | sort \\\n",
    "    | python userReducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full execution on HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `./output-mean': No such file or directory\n",
      "19/04/08 15:59:12 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [./meanGenreMapper.py, ./meanGenreReducer.py, ./movies.csv] [/usr/hdp/2.6.5.0-292/hadoop-mapreduce/hadoop-streaming-2.7.3.2.6.5.0-292.jar] /hadoop_java_io_tmpdir/streamjob2141192298957538711.jar tmpDir=null\n",
      "19/04/08 15:59:13 INFO client.AHSProxy: Connecting to Application History server at dscim003.palmetto.clemson.edu/10.125.8.215:10200\n",
      "19/04/08 15:59:13 INFO client.AHSProxy: Connecting to Application History server at dscim003.palmetto.clemson.edu/10.125.8.215:10200\n",
      "19/04/08 15:59:13 INFO hdfs.DFSClient: Created HDFS_DELEGATION_TOKEN token 22171 for yuzhey on ha-hdfs:dsci\n",
      "19/04/08 15:59:13 INFO security.TokenCache: Got dt for hdfs://dsci; Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:dsci, Ident: (HDFS_DELEGATION_TOKEN token 22171 for yuzhey)\n",
      "19/04/08 15:59:13 INFO lzo.GPLNativeCodeLoader: Loaded native gpl library\n",
      "19/04/08 15:59:13 INFO lzo.LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev 77914d73bfc2e32253ff2bb7c61d03eaca973704]\n",
      "19/04/08 15:59:13 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "19/04/08 15:59:13 INFO mapreduce.JobSubmitter: number of splits:5\n",
      "19/04/08 15:59:13 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1553277363335_0350\n",
      "19/04/08 15:59:13 INFO mapreduce.JobSubmitter: Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:dsci, Ident: (HDFS_DELEGATION_TOKEN token 22171 for yuzhey)\n",
      "19/04/08 15:59:14 INFO impl.TimelineClientImpl: Timeline service address: http://dscim003.palmetto.clemson.edu:8188/ws/v1/timeline/\n",
      "19/04/08 15:59:14 INFO impl.YarnClientImpl: Submitted application application_1553277363335_0350\n",
      "19/04/08 15:59:14 INFO mapreduce.Job: The url to track the job: http://dscim001.palmetto.clemson.edu:8088/proxy/application_1553277363335_0350/\n",
      "19/04/08 15:59:14 INFO mapreduce.Job: Running job: job_1553277363335_0350\n",
      "19/04/08 15:59:22 INFO mapreduce.Job: Job job_1553277363335_0350 running in uber mode : false\n",
      "19/04/08 15:59:22 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "19/04/08 15:59:34 INFO mapreduce.Job:  map 30% reduce 0%\n",
      "19/04/08 15:59:37 INFO mapreduce.Job:  map 42% reduce 0%\n",
      "19/04/08 15:59:39 INFO mapreduce.Job:  map 44% reduce 0%\n",
      "19/04/08 15:59:40 INFO mapreduce.Job:  map 66% reduce 0%\n",
      "19/04/08 15:59:41 INFO mapreduce.Job:  map 88% reduce 0%\n",
      "19/04/08 15:59:42 INFO mapreduce.Job:  map 91% reduce 0%\n",
      "19/04/08 15:59:44 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "19/04/08 15:59:44 INFO mapreduce.Job: Job job_1553277363335_0350 completed successfully\n",
      "19/04/08 15:59:44 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=6105\n",
      "\t\tFILE: Number of bytes written=1013405\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=663945432\n",
      "\t\tHDFS: Number of bytes written=2224\n",
      "\t\tHDFS: Number of read operations=18\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=6\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tRack-local map tasks=4\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=262575\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=6402\n",
      "\t\tTotal time spent by all map tasks (ms)=87525\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2134\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=87525\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2134\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=1128372300\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=27511528\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=24404097\n",
      "\t\tMap output records=100\n",
      "\t\tMap output bytes=5899\n",
      "\t\tMap output materialized bytes=6129\n",
      "\t\tInput split bytes=480\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=20\n",
      "\t\tReduce shuffle bytes=6129\n",
      "\t\tReduce input records=100\n",
      "\t\tReduce output records=100\n",
      "\t\tSpilled Records=200\n",
      "\t\tShuffled Maps =5\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=5\n",
      "\t\tGC time elapsed (ms)=6959\n",
      "\t\tCPU time spent (ms)=277220\n",
      "\t\tPhysical memory (bytes) snapshot=13763350528\n",
      "\t\tVirtual memory (bytes) snapshot=79790452736\n",
      "\t\tTotal committed heap usage (bytes)=15728640000\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=663944952\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2224\n",
      "19/04/08 15:59:44 INFO streaming.StreamJob: Output directory: ./output-meanGenre\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -R ./output-meanGenre\n",
    "!yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\\n",
    "    -input /repository/movielens/ratings.csv \\\n",
    "    -output ./output-meanGenre \\\n",
    "    -file ./meanGenreMapper.py \\\n",
    "    -mapper meanGenreMapper.py \\\n",
    "    -file ./meanGenreReducer.py \\\n",
    "    -reducer meanGenreReducer.py \\\n",
    "    -file ./movies.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(no genres listed)\t3.33320433437\n",
      "(no genres listed)\t3.24496056091\n",
      "(no genres listed)\t3.13632514818\n",
      "(no genres listed)\t3.24122807018\n",
      "(no genres listed)\t3.06960352423\n",
      "Action\t3.44444281044\n",
      "Action\t3.45894409618\n",
      "Action\t3.45845824548\n",
      "Action\t3.45852368671\n",
      "Action\t3.45127443462\n",
      "Adventure\t3.49949764004\n",
      "Adventure\t3.51170577777\n",
      "Adventure\t3.50045914479\n",
      "Adventure\t3.51365006298\n",
      "Adventure\t3.50983028992\n",
      "Animation\t3.61306183751\n",
      "Animation\t3.60252051056\n",
      "Animation\t3.59798601506\n",
      "Animation\t3.61029521137\n",
      "Animation\t3.62827657548\n",
      "Children\t3.43086201442\n",
      "Children\t3.40942305911\n",
      "Children\t3.41414288185\n",
      "Children\t3.4076072136\n",
      "Children\t3.42058416433\n",
      "Comedy\t3.42433055487\n",
      "Comedy\t3.4145183972\n",
      "Comedy\t3.4240967791\n",
      "Comedy\t3.41294742788\n",
      "Comedy\t3.41103262748\n",
      "Crime\t3.67117984465\n",
      "Crime\t3.68252001502\n",
      "Crime\t3.68327146944\n",
      "Crime\t3.68051272167\n",
      "Crime\t3.67464524153\n",
      "Documentary\t3.70401121754\n",
      "Documentary\t3.72438481388\n",
      "Documentary\t3.7307135799\n",
      "Documentary\t3.73771762654\n",
      "Documentary\t3.71807109665\n",
      "Drama\t3.67062916815\n",
      "Drama\t3.67299605853\n",
      "Drama\t3.66980712371\n",
      "Drama\t3.67826043418\n",
      "Drama\t3.679617718\n",
      "Fantasy\t3.51467193624\n",
      "Fantasy\t3.49397514261\n",
      "Fantasy\t3.50104116596\n",
      "Fantasy\t3.49342761127\n",
      "Fantasy\t3.5112956598\n",
      "Film-Noir\t3.94211319996\n",
      "Film-Noir\t3.93615338177\n",
      "Film-Noir\t3.94425134206\n",
      "Film-Noir\t3.93200114188\n",
      "Film-Noir\t3.95015418409\n",
      "Horror\t3.25448729729\n",
      "Horror\t3.28432572491\n",
      "Horror\t3.28701293883\n",
      "Horror\t3.27370501473\n",
      "Horror\t3.27581586232\n",
      "IMAX\t3.6223382545\n",
      "IMAX\t3.6452719762\n",
      "IMAX\t3.6297075954\n",
      "IMAX\t3.64507861476\n",
      "IMAX\t3.64295957885\n",
      "Musical\t3.5344034885\n",
      "Musical\t3.53381135657\n",
      "Musical\t3.53932523629\n",
      "Musical\t3.55182630207\n",
      "Musical\t3.55929003747\n",
      "Mystery\t3.67001636455\n",
      "Mystery\t3.65411453591\n",
      "Mystery\t3.65772155812\n",
      "Mystery\t3.65721539948\n",
      "Mystery\t3.66826478773\n",
      "Romance\t3.54741516242\n",
      "Romance\t3.53622908687\n",
      "Romance\t3.54881184626\n",
      "Romance\t3.53906975769\n",
      "Romance\t3.54060962351\n",
      "Sci-Fi\t3.44652366992\n",
      "Sci-Fi\t3.4600744283\n",
      "Sci-Fi\t3.45936808689\n",
      "Sci-Fi\t3.4590160275\n",
      "Sci-Fi\t3.45037814875\n",
      "Thriller\t3.50945653733\n",
      "Thriller\t3.51859726273\n",
      "Thriller\t3.5040119679\n",
      "Thriller\t3.51710613615\n",
      "Thriller\t3.51382639677\n",
      "War\t3.80523318733\n",
      "War\t3.8007199102\n",
      "War\t3.79621832198\n",
      "War\t3.80244254285\n",
      "War\t3.8116217732\n",
      "Western\t3.57519710712\n",
      "Western\t3.57103886159\n",
      "Western\t3.57276515729\n",
      "Western\t3.58101835248\n",
      "Western\t3.55877749719\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat ./output-meanGenre/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/04/09 13:31:53 INFO fs.TrashPolicyDefault: Moved: 'hdfs://dsci/user/yuzhey/output-genre' to trash at: hdfs://dsci/user/yuzhey/.Trash/Current/user/yuzhey/output-genre1554831113258\n",
      "19/04/09 13:31:54 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [./genreMapper.py, ./genreReducer.py, ./movies.csv] [/usr/hdp/2.6.5.0-292/hadoop-mapreduce/hadoop-streaming-2.7.3.2.6.5.0-292.jar] /hadoop_java_io_tmpdir/streamjob934711794381139792.jar tmpDir=null\n",
      "19/04/09 13:31:55 INFO client.AHSProxy: Connecting to Application History server at dscim003.palmetto.clemson.edu/10.125.8.215:10200\n",
      "19/04/09 13:31:55 INFO client.AHSProxy: Connecting to Application History server at dscim003.palmetto.clemson.edu/10.125.8.215:10200\n",
      "19/04/09 13:31:55 INFO hdfs.DFSClient: Created HDFS_DELEGATION_TOKEN token 22226 for yuzhey on ha-hdfs:dsci\n",
      "19/04/09 13:31:55 INFO security.TokenCache: Got dt for hdfs://dsci; Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:dsci, Ident: (HDFS_DELEGATION_TOKEN token 22226 for yuzhey)\n",
      "19/04/09 13:31:55 INFO lzo.GPLNativeCodeLoader: Loaded native gpl library\n",
      "19/04/09 13:31:55 INFO lzo.LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev 77914d73bfc2e32253ff2bb7c61d03eaca973704]\n",
      "19/04/09 13:31:55 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "19/04/09 13:31:55 INFO mapreduce.JobSubmitter: number of splits:5\n",
      "19/04/09 13:31:56 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1553277363335_0398\n",
      "19/04/09 13:31:56 INFO mapreduce.JobSubmitter: Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:dsci, Ident: (HDFS_DELEGATION_TOKEN token 22226 for yuzhey)\n",
      "19/04/09 13:31:56 INFO impl.TimelineClientImpl: Timeline service address: http://dscim003.palmetto.clemson.edu:8188/ws/v1/timeline/\n",
      "19/04/09 13:31:56 INFO impl.YarnClientImpl: Submitted application application_1553277363335_0398\n",
      "19/04/09 13:31:57 INFO mapreduce.Job: The url to track the job: http://dscim001.palmetto.clemson.edu:8088/proxy/application_1553277363335_0398/\n",
      "19/04/09 13:31:57 INFO mapreduce.Job: Running job: job_1553277363335_0398\n",
      "19/04/09 13:32:05 INFO mapreduce.Job: Job job_1553277363335_0398 running in uber mode : false\n",
      "19/04/09 13:32:05 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "19/04/09 13:32:11 INFO mapreduce.Job: Task Id : attempt_1553277363335_0398_m_000003_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:322)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:535)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:453)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:170)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1869)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:164)\n",
      "\n",
      "Container killed by the ApplicationMaster.\n",
      "Container killed on request. Exit code is 143\n",
      "Container exited with a non-zero exit code 143. \n",
      "\n",
      "19/04/09 13:32:11 INFO mapreduce.Job: Task Id : attempt_1553277363335_0398_m_000000_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:322)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:535)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:453)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:170)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1869)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:164)\n",
      "\n",
      "Container killed by the ApplicationMaster.\n",
      "Container killed on request. Exit code is 143\n",
      "Container exited with a non-zero exit code 143. \n",
      "\n",
      "19/04/09 13:32:11 INFO mapreduce.Job: Task Id : attempt_1553277363335_0398_m_000001_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:322)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:535)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:453)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:170)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1869)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:164)\n",
      "\n",
      "Container killed by the ApplicationMaster.\n",
      "Container killed on request. Exit code is 143\n",
      "Container exited with a non-zero exit code 143. \n",
      "\n",
      "19/04/09 13:32:11 INFO mapreduce.Job: Task Id : attempt_1553277363335_0398_m_000004_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:322)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:535)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:453)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:170)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1869)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:164)\n",
      "\n",
      "Container killed by the ApplicationMaster.\n",
      "Container killed on request. Exit code is 143\n",
      "Container exited with a non-zero exit code 143. \n",
      "\n",
      "19/04/09 13:32:12 INFO mapreduce.Job: Task Id : attempt_1553277363335_0398_m_000002_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:322)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:535)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:453)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:170)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1869)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:164)\n",
      "\n",
      "19/04/09 13:32:17 INFO mapreduce.Job: Task Id : attempt_1553277363335_0398_m_000002_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:322)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:535)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:453)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:170)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1869)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:164)\n",
      "\n",
      "Container killed by the ApplicationMaster.\n",
      "Container killed on request. Exit code is 143\n",
      "Container exited with a non-zero exit code 143. \n",
      "\n",
      "19/04/09 13:32:17 INFO mapreduce.Job: Task Id : attempt_1553277363335_0398_m_000001_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:322)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:535)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:453)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:170)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1869)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:164)\n",
      "\n",
      "Container killed by the ApplicationMaster.\n",
      "Container killed on request. Exit code is 143\n",
      "Container exited with a non-zero exit code 143. \n",
      "\n",
      "19/04/09 13:32:17 INFO mapreduce.Job: Task Id : attempt_1553277363335_0398_m_000003_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:322)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:535)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:453)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:170)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1869)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:164)\n",
      "\n",
      "Container killed by the ApplicationMaster.\n",
      "Container killed on request. Exit code is 143\n",
      "Container exited with a non-zero exit code 143. \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/04/09 13:32:18 INFO mapreduce.Job: Task Id : attempt_1553277363335_0398_m_000000_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:322)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:535)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:453)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:170)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1869)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:164)\n",
      "\n",
      "Container killed by the ApplicationMaster.\n",
      "Container killed on request. Exit code is 143\n",
      "Container exited with a non-zero exit code 143. \n",
      "\n",
      "19/04/09 13:32:18 INFO mapreduce.Job: Task Id : attempt_1553277363335_0398_m_000004_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:322)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:535)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:453)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:170)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1869)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:164)\n",
      "\n",
      "19/04/09 13:32:22 INFO mapreduce.Job: Task Id : attempt_1553277363335_0398_m_000002_2, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:322)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:535)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:453)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:170)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1869)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:164)\n",
      "\n",
      "Container killed by the ApplicationMaster.\n",
      "Container killed on request. Exit code is 143\n",
      "Container exited with a non-zero exit code 143. \n",
      "\n",
      "19/04/09 13:32:22 INFO mapreduce.Job: Task Id : attempt_1553277363335_0398_m_000001_2, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:322)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:535)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:453)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:170)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1869)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:164)\n",
      "\n",
      "Container killed by the ApplicationMaster.\n",
      "Container killed on request. Exit code is 143\n",
      "Container exited with a non-zero exit code 143. \n",
      "\n",
      "19/04/09 13:32:23 INFO mapreduce.Job: Task Id : attempt_1553277363335_0398_m_000000_2, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:322)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:535)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:453)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:170)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1869)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:164)\n",
      "\n",
      "Container killed by the ApplicationMaster.\n",
      "Container killed on request. Exit code is 143\n",
      "Container exited with a non-zero exit code 143. \n",
      "\n",
      "19/04/09 13:32:23 INFO mapreduce.Job: Task Id : attempt_1553277363335_0398_m_000004_2, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:322)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:535)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:453)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:170)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1869)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:164)\n",
      "\n",
      "Container killed by the ApplicationMaster.\n",
      "Container killed on request. Exit code is 143\n",
      "Container exited with a non-zero exit code 143. \n",
      "\n",
      "19/04/09 13:32:23 INFO mapreduce.Job: Task Id : attempt_1553277363335_0398_m_000003_2, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:322)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:535)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:453)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:170)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1869)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:164)\n",
      "\n",
      "Container killed by the ApplicationMaster.\n",
      "Container killed on request. Exit code is 143\n",
      "Container exited with a non-zero exit code 143. \n",
      "\n",
      "19/04/09 13:32:27 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "19/04/09 13:32:27 INFO mapreduce.Job: Job job_1553277363335_0398 failed with state FAILED due to: Task failed task_1553277363335_0398_m_000002\n",
      "Job failed as tasks failed. failedMaps:1 failedReduces:0\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/04/09 13:32:27 INFO mapreduce.Job: Counters: 18\r\n",
      "\tJob Counters \r\n",
      "\t\tFailed map tasks=16\r\n",
      "\t\tKilled map tasks=4\r\n",
      "\t\tKilled reduce tasks=1\r\n",
      "\t\tLaunched map tasks=20\r\n",
      "\t\tOther local map tasks=15\r\n",
      "\t\tData-local map tasks=1\r\n",
      "\t\tRack-local map tasks=4\r\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=238470\r\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\r\n",
      "\t\tTotal time spent by all map tasks (ms)=79490\r\n",
      "\t\tTotal time spent by all reduce tasks (ms)=0\r\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=79490\r\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=0\r\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=1024785080\r\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=0\r\n",
      "\tMap-Reduce Framework\r\n",
      "\t\tCPU time spent (ms)=0\r\n",
      "\t\tPhysical memory (bytes) snapshot=0\r\n",
      "\t\tVirtual memory (bytes) snapshot=0\r\n",
      "19/04/09 13:32:27 ERROR streaming.StreamJob: Job not successful!\r\n",
      "Streaming Command Failed!\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -R ./output-genre\n",
    "!yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\\n",
    "    -input /repository/movielens/ratings.csv \\\n",
    "    -output ./output-genre \\\n",
    "    -file ./genreMapper.py \\\n",
    "    -mapper genreMapper.py \\\n",
    "    -file ./genreReducer.py \\\n",
    "    -reducer genreReducer.py \\\n",
    "    -file ./movies.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/04/08 15:54:59 INFO fs.TrashPolicyDefault: Moved: 'hdfs://dsci/user/yuzhey/output-user' to trash at: hdfs://dsci/user/yuzhey/.Trash/Current/user/yuzhey/output-user\n",
      "19/04/08 15:55:00 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [./userMapper.py, ./userReducer.py, ./movies.csv] [/usr/hdp/2.6.5.0-292/hadoop-mapreduce/hadoop-streaming-2.7.3.2.6.5.0-292.jar] /hadoop_java_io_tmpdir/streamjob2279689388552816955.jar tmpDir=null\n",
      "19/04/08 15:55:01 INFO client.AHSProxy: Connecting to Application History server at dscim003.palmetto.clemson.edu/10.125.8.215:10200\n",
      "19/04/08 15:55:01 INFO client.AHSProxy: Connecting to Application History server at dscim003.palmetto.clemson.edu/10.125.8.215:10200\n",
      "19/04/08 15:55:01 INFO hdfs.DFSClient: Created HDFS_DELEGATION_TOKEN token 22165 for yuzhey on ha-hdfs:dsci\n",
      "19/04/08 15:55:01 INFO security.TokenCache: Got dt for hdfs://dsci; Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:dsci, Ident: (HDFS_DELEGATION_TOKEN token 22165 for yuzhey)\n",
      "19/04/08 15:55:02 INFO lzo.GPLNativeCodeLoader: Loaded native gpl library\n",
      "19/04/08 15:55:02 INFO lzo.LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev 77914d73bfc2e32253ff2bb7c61d03eaca973704]\n",
      "19/04/08 15:55:02 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "19/04/08 15:55:02 INFO mapreduce.JobSubmitter: number of splits:5\n",
      "19/04/08 15:55:02 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1553277363335_0345\n",
      "19/04/08 15:55:02 INFO mapreduce.JobSubmitter: Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:dsci, Ident: (HDFS_DELEGATION_TOKEN token 22165 for yuzhey)\n",
      "19/04/08 15:55:02 INFO impl.TimelineClientImpl: Timeline service address: http://dscim003.palmetto.clemson.edu:8188/ws/v1/timeline/\n",
      "19/04/08 15:55:03 INFO impl.YarnClientImpl: Submitted application application_1553277363335_0345\n",
      "19/04/08 15:55:03 INFO mapreduce.Job: The url to track the job: http://dscim001.palmetto.clemson.edu:8088/proxy/application_1553277363335_0345/\n",
      "19/04/08 15:55:03 INFO mapreduce.Job: Running job: job_1553277363335_0345\n",
      "19/04/08 15:55:09 INFO mapreduce.Job: Job job_1553277363335_0345 running in uber mode : false\n",
      "19/04/08 15:55:09 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "19/04/08 15:55:21 INFO mapreduce.Job:  map 9% reduce 0%\n",
      "19/04/08 15:55:22 INFO mapreduce.Job:  map 12% reduce 0%\n",
      "19/04/08 15:55:23 INFO mapreduce.Job:  map 15% reduce 0%\n",
      "19/04/08 15:55:24 INFO mapreduce.Job:  map 24% reduce 0%\n",
      "19/04/08 15:55:25 INFO mapreduce.Job:  map 26% reduce 0%\n",
      "19/04/08 15:55:26 INFO mapreduce.Job:  map 29% reduce 0%\n",
      "19/04/08 15:55:27 INFO mapreduce.Job:  map 35% reduce 0%\n",
      "19/04/08 15:55:28 INFO mapreduce.Job:  map 38% reduce 0%\n",
      "19/04/08 15:55:29 INFO mapreduce.Job:  map 42% reduce 0%\n",
      "19/04/08 15:55:30 INFO mapreduce.Job:  map 48% reduce 0%\n",
      "19/04/08 15:55:31 INFO mapreduce.Job:  map 57% reduce 0%\n",
      "19/04/08 15:55:32 INFO mapreduce.Job:  map 62% reduce 0%\n",
      "19/04/08 15:55:33 INFO mapreduce.Job:  map 73% reduce 0%\n",
      "19/04/08 15:55:35 INFO mapreduce.Job:  map 91% reduce 0%\n",
      "19/04/08 15:55:38 INFO mapreduce.Job:  map 93% reduce 0%\n",
      "19/04/08 15:55:39 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "19/04/08 15:55:41 INFO mapreduce.Job:  map 100% reduce 74%\n",
      "19/04/08 15:55:43 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "19/04/08 15:55:44 INFO mapreduce.Job: Job job_1553277363335_0345 completed successfully\n",
      "19/04/08 15:55:44 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=60711520\n",
      "\t\tFILE: Number of bytes written=122423905\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=663945432\n",
      "\t\tHDFS: Number of bytes written=72\n",
      "\t\tHDFS: Number of read operations=18\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=6\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=1\n",
      "\t\tRack-local map tasks=5\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=369378\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=30678\n",
      "\t\tTotal time spent by all map tasks (ms)=123126\n",
      "\t\tTotal time spent by all reduce tasks (ms)=10226\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=123126\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=10226\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=1587340392\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=131833592\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=24404097\n",
      "\t\tMap output records=259141\n",
      "\t\tMap output bytes=59864301\n",
      "\t\tMap output materialized bytes=60711544\n",
      "\t\tInput split bytes=480\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=259137\n",
      "\t\tReduce shuffle bytes=60711544\n",
      "\t\tReduce input records=259141\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=518282\n",
      "\t\tShuffled Maps =5\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=5\n",
      "\t\tGC time elapsed (ms)=8957\n",
      "\t\tCPU time spent (ms)=358750\n",
      "\t\tPhysical memory (bytes) snapshot=13577818112\n",
      "\t\tVirtual memory (bytes) snapshot=79741067264\n",
      "\t\tTotal committed heap usage (bytes)=15591276544\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=663944952\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=72\n",
      "19/04/08 15:55:44 INFO streaming.StreamJob: Output directory: ./output-user\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -R ./output-user\n",
    "!yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\\n",
    "    -input /repository/movielens/ratings.csv \\\n",
    "    -output ./output-user \\\n",
    "    -file ./userMapper.py \\\n",
    "    -mapper userMapper.py \\\n",
    "    -file ./userReducer.py \\\n",
    "    -reducer userReducer.py \\\n",
    "    -file ./movies.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186590 -- Total Rating Counts: 13250 -- Most Rated Genre: Drama - 8026\t\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat ./output-user/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Anaconda 5.1.0)",
   "language": "python",
   "name": "anaconda3-5.1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
